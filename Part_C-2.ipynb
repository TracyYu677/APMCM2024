{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import kaiwu as kw\n",
    "kw.license.init(user_id=\"72317291601100802\", sdk_code=\"vDSsMrcS1XvoHxrKEyWGPu3y6bydtx\")\n",
    "from kaiwu.classical import SimulatedAnnealingOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define a simple CNN model for CIFAR-10\n",
    "def build_cnn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = build_cnn_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (for initial weights)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Extract the trained weights (kernel and bias) from the first Conv2D layer\n",
    "conv2d_layer = model.layers[0]  # Assuming the first Conv2D layer\n",
    "kernel_weights, bias_weights = conv2d_layer.get_weights()  # Extract kernel and bias\n",
    "\n",
    "# binarized_kernel_weights = np.sign(kernel_weights)  # Binarize the kernel weights to +1 or -1\n",
    "# bias_weights will be left unchanged in this case\n",
    "binarized_kernel_weights = kernel_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.5455 - accuracy: 0.4370 - val_loss: 1.2772 - val_accuracy: 0.5337\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.1698 - accuracy: 0.5839 - val_loss: 1.1169 - val_accuracy: 0.6003\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.0002 - accuracy: 0.6495 - val_loss: 1.0429 - val_accuracy: 0.6344\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.8859 - accuracy: 0.6909 - val_loss: 0.9807 - val_accuracy: 0.6611\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.8103 - accuracy: 0.7157 - val_loss: 0.9475 - val_accuracy: 0.6688\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define a simple CNN model for CIFAR-10\n",
    "def build_cnn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = build_cnn_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (for initial weights)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Extract the trained weights (kernel and bias) from the first Conv2D layer\n",
    "conv2d_layer = model.layers[0]  # Assuming the first Conv2D layer\n",
    "kernel_weights, bias_weights = conv2d_layer.get_weights()  # Extract kernel and bias\n",
    "\n",
    "# binarized_kernel_weights = np.sign(kernel_weights)  # Binarize the kernel weights to +1 or -1\n",
    "# bias_weights will be left unchanged in this case\n",
    "binarized_kernel_weights = kernel_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Test Set Accuracy: 66.88%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)  # x_test is the CIFAR-10 test data\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # Get the index with the highest probability for each sample\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)  # Compare predicted vs. true labels\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness Test (CNN): Accuracy on noisy data = 50.47%\n"
     ]
    }
   ],
   "source": [
    "def test_robustness_cnn(model, x_test, y_test, noise_factor=0.1):\n",
    "    \"\"\"\n",
    "    Tests the robustness of the CNN by adding random noise to the test data.\n",
    "    \"\"\"\n",
    "    # Add random noise to the test data\n",
    "    x_test_noisy = x_test + noise_factor * np.random.normal(0, 1, x_test.shape)\n",
    "    x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)  # Ensure values are within [0, 1]\n",
    "\n",
    "    # Evaluate the model on noisy test data\n",
    "    test_loss_noisy, test_acc_noisy = model.evaluate(x_test_noisy, y_test, verbose=0)\n",
    "    print(f\"Robustness Test (CNN): Accuracy on noisy data = {test_acc_noisy * 100:.2f}%\")\n",
    "    \n",
    "    return test_acc_noisy\n",
    "\n",
    "# Example usage:\n",
    "cnn_accuracy_noisy = test_robustness_cnn(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qubo_matrix(weights,idx):\n",
    "    # num_weights = len(weights)\n",
    "    num_weights = 600\n",
    "    Q = np.zeros((num_weights, num_weights))\n",
    "    \n",
    "    # Penalize large weights and encourage sparsity\n",
    "    for i in range(num_weights):\n",
    "        Q[i, i] = abs(weights[i+idx])  # Larger weights are penalized more\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "initial_weights = []\n",
    "initial_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        kernel, bias = layer.get_weights()\n",
    "        initial_weights.append(kernel.flatten())  # Flatten kernels (weights)\n",
    "        initial_biases.append(bias.flatten())    # Flatten biases\n",
    "\n",
    "# Concatenate the initial weights and biases\n",
    "initial_weights = np.concatenate(initial_weights)\n",
    "initial_biases = np.concatenate(initial_biases)\n",
    "\n",
    "binary_weights_all = np.zeros(1)\n",
    "for i in range(int(initial_weights.shape[0] / 600)):\n",
    "    idx = i*600\n",
    "    Q = Q = generate_qubo_matrix(initial_weights,idx)\n",
    "    solver = SimulatedAnnealingOptimizer()\n",
    "\n",
    "    # Solve the QUBO problem\n",
    "    solution = solver.solve(Q)\n",
    "    best_solution = solution[0]\n",
    "    \n",
    "    optimized_x = best_solution\n",
    "    binary_weights = np.zeros(600)\n",
    "    binary_weights[optimized_x == 1] = initial_weights[i*600:i*600+600][optimized_x == 1]\n",
    "    binary_weights_all = np.concatenate((binary_weights_all, binary_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_weights_all_new = binary_weights_all[1:]\n",
    "binary_weights_all_new = np.concatenate((binary_weights_all_new, np.array([0] * (len(initial_weights) - len(binary_weights_all_new)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356810 (1.36 MB)\n",
      "Trainable params: 356810 (1.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct weights and biases for each layer\n",
    "layer_idx_w = 0\n",
    "layer_idx_b = 0\n",
    "updated_weights = []\n",
    "updated_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        num_params = layer.get_weights()[0].size  # Kernel size\n",
    "        num_biases = layer.get_weights()[1].size  # Bias size\n",
    "\n",
    "        # Get the kernel weights for the layer\n",
    "        layer_kernel = binary_weights_all_new[layer_idx_w:layer_idx_w + num_params].reshape(layer.get_weights()[0].shape)\n",
    "        updated_weights.append(layer_kernel)\n",
    "\n",
    "        # Get the biases for the layer\n",
    "        # layer_bias = binary_weights_all_new[layer_idx + num_params:layer_idx + num_params + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        layer_bias = initial_biases[layer_idx_b:layer_idx_b + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        updated_biases.append(layer_bias)\n",
    "\n",
    "        # Move to the next set of weights\n",
    "        # layer_idx += num_params + num_biases\n",
    "        layer_idx_w += num_params\n",
    "        layer_idx_b += num_biases\n",
    "\n",
    "# Now set the weights and biases to the model\n",
    "updated_weights = [w for w in updated_weights]\n",
    "updated_biases = [b for b in updated_biases]\n",
    "\n",
    "# Set the new weights and biases for the model\n",
    "model.set_weights([w for pair in zip(updated_weights, updated_biases) for w in pair])\n",
    "\n",
    "# Verify the model summary after setting the weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.0850 - accuracy: 0.6164 - val_loss: 0.9775 - val_accuracy: 0.6595\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.8193 - accuracy: 0.7144 - val_loss: 0.8689 - val_accuracy: 0.6934\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.7117 - accuracy: 0.7517 - val_loss: 0.8322 - val_accuracy: 0.7081\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.6484 - accuracy: 0.7723 - val_loss: 0.8026 - val_accuracy: 0.7248\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.5761 - accuracy: 0.7992 - val_loss: 0.8128 - val_accuracy: 0.7248\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.8128 - accuracy: 0.7248\n",
      "Test accuracy: 72.48%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness Test (CNN): Accuracy on noisy data = 57.59%\n"
     ]
    }
   ],
   "source": [
    "cnn_accuracy_noisy = test_robustness_cnn(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define the deep learning model (a simple CNN)\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a function to create a more complex QUBO matrix\n",
    "\n",
    "\n",
    "def create_complex_qubo(weights, idx,sparsity_factor=0.1, interaction_factor=0.01, entropy_factor=0.1):\n",
    "    # n_weights = len(weights)\n",
    "    n_weights = 600\n",
    "    Q = np.zeros((n_weights, n_weights))\n",
    "    \n",
    "    # Add sparsity regularization (penalize large weights, encourage sparsity)\n",
    "    for i in range(n_weights):\n",
    "        Q[i, i] += sparsity_factor * weights[i+idx] ** 2  # Penalize non-zero weights\n",
    "    \n",
    "    # Add interaction regularization (encourage some weights to be correlated or anticorrelated)\n",
    "    for i in range(n_weights):\n",
    "        for j in range(i + 1, n_weights):\n",
    "            Q[i, j] += interaction_factor * weights[i+idx] * weights[j+idx]  # Penalize uncorrelated weights\n",
    "    \n",
    "    # Add entropy minimization (encourage confident predictions)\n",
    "    for i in range(n_weights):\n",
    "        for j in range(i + 1, n_weights):\n",
    "            Q[i, j] += entropy_factor * (weights[i+idx] - 0.5) * (weights[j+idx] - 0.5)  # Enforce weights to be either 0 or 1\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Initialize the model and get the weights\n",
    "model = create_model()\n",
    "initial_weights = []\n",
    "initial_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        kernel, bias = layer.get_weights()\n",
    "        initial_weights.append(kernel.flatten())  # Flatten kernels (weights)\n",
    "        initial_biases.append(bias.flatten())    # Flatten biases\n",
    "\n",
    "# Concatenate the initial weights and biases\n",
    "initial_weights = np.concatenate(initial_weights)\n",
    "initial_biases = np.concatenate(initial_biases)\n",
    "\n",
    "\n",
    "# Create a more complex QUBO matrix\n",
    "binary_weights_all = np.zeros(1)\n",
    "for i in range(int(initial_weights.shape[0] / 600)):\n",
    "    idx = i*600\n",
    "    Q = create_complex_qubo(initial_weights,idx)\n",
    "    solver = SimulatedAnnealingOptimizer()\n",
    "\n",
    "    # Solve the QUBO problem\n",
    "    solution = solver.solve(Q)\n",
    "    best_solution = solution[0]\n",
    "    \n",
    "    optimized_x = best_solution\n",
    "    binary_weights = np.zeros(600)\n",
    "    binary_weights[optimized_x == 1] = initial_weights[i*600:i*600+600][optimized_x == 1]\n",
    "    binary_weights_all = np.concatenate((binary_weights_all, binary_weights))\n",
    "\n",
    "binary_weights_all_new = binary_weights_all[1:]\n",
    "binary_weights_all_new = np.concatenate((binary_weights_all_new, np.array([0] * (len(initial_weights) - len(binary_weights_all_new)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 315722 (1.20 MB)\n",
      "Trainable params: 315722 (1.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct weights and biases for each layer\n",
    "layer_idx_w = 0\n",
    "layer_idx_b = 0\n",
    "updated_weights = []\n",
    "updated_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        num_params = layer.get_weights()[0].size  # Kernel size\n",
    "        num_biases = layer.get_weights()[1].size  # Bias size\n",
    "\n",
    "        # Get the kernel weights for the layer\n",
    "        layer_kernel = binary_weights_all_new[layer_idx_w:layer_idx_w + num_params].reshape(layer.get_weights()[0].shape)\n",
    "        updated_weights.append(layer_kernel)\n",
    "\n",
    "        # Get the biases for the layer\n",
    "        # layer_bias = binary_weights_all_new[layer_idx + num_params:layer_idx + num_params + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        layer_bias = initial_biases[layer_idx_b:layer_idx_b + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        updated_biases.append(layer_bias)\n",
    "\n",
    "        # Move to the next set of weights\n",
    "        # layer_idx += num_params + num_biases\n",
    "        layer_idx_w += num_params\n",
    "        layer_idx_b += num_biases\n",
    "\n",
    "# Now set the weights and biases to the model\n",
    "updated_weights = [w for w in updated_weights]\n",
    "updated_biases = [b for b in updated_biases]\n",
    "\n",
    "# Set the new weights and biases for the model\n",
    "model.set_weights([w for pair in zip(updated_weights, updated_biases) for w in pair])\n",
    "\n",
    "# Verify the model summary after setting the weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.6383 - accuracy: 0.3953 - val_loss: 1.3699 - val_accuracy: 0.5031\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.3053 - accuracy: 0.5338 - val_loss: 1.2174 - val_accuracy: 0.5742\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.1414 - accuracy: 0.5979 - val_loss: 1.0696 - val_accuracy: 0.6296\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.0310 - accuracy: 0.6406 - val_loss: 1.0255 - val_accuracy: 0.6466\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9503 - accuracy: 0.6676 - val_loss: 1.0283 - val_accuracy: 0.6442\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8883 - accuracy: 0.6918 - val_loss: 0.9249 - val_accuracy: 0.6790\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8272 - accuracy: 0.7117 - val_loss: 0.9096 - val_accuracy: 0.6871\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7787 - accuracy: 0.7283 - val_loss: 0.8999 - val_accuracy: 0.6956\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7352 - accuracy: 0.7442 - val_loss: 0.9146 - val_accuracy: 0.6841\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.7606 - val_loss: 0.9021 - val_accuracy: 0.6937\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.9021 - accuracy: 0.6937\n",
      "Test accuracy: 69.37%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness Test (CNN): Accuracy on noisy data = 62.17%\n"
     ]
    }
   ],
   "source": [
    "cnn_accuracy_noisy = test_robustness_cnn(model, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
