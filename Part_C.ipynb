{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import kaiwu as kw\n",
    "kw.license.init(user_id=\"72317291601100802\", sdk_code=\"vDSsMrcS1XvoHxrKEyWGPu3y6bydtx\")\n",
    "from kaiwu.classical import SimulatedAnnealingOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4954 - accuracy: 0.4618 - val_loss: 1.3843 - val_accuracy: 0.5197\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.1419 - accuracy: 0.5999 - val_loss: 1.0623 - val_accuracy: 0.6301\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.9931 - accuracy: 0.6543 - val_loss: 0.9822 - val_accuracy: 0.6578\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.9002 - accuracy: 0.6862 - val_loss: 0.9449 - val_accuracy: 0.6705\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.8374 - accuracy: 0.7087 - val_loss: 0.9362 - val_accuracy: 0.6770\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define a simple CNN model for CIFAR-10\n",
    "def build_cnn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = build_cnn_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (for initial weights)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Extract the trained weights (kernel and bias) from the first Conv2D layer\n",
    "conv2d_layer = model.layers[0]  # Assuming the first Conv2D layer\n",
    "kernel_weights, bias_weights = conv2d_layer.get_weights()  # Extract kernel and bias\n",
    "\n",
    "# binarized_kernel_weights = np.sign(kernel_weights)  # Binarize the kernel weights to +1 or -1\n",
    "# bias_weights will be left unchanged in this case\n",
    "binarized_kernel_weights = kernel_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Test Set Accuracy: 67.70%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)  # x_test is the CIFAR-10 test data\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # Get the index with the highest probability for each sample\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)  # Compare predicted vs. true labels\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 600)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_qubo_matrix(weights):\n",
    "    num_weights = len(weights)\n",
    "    Q = np.zeros((num_weights, num_weights))\n",
    "    \n",
    "    # Penalize large weights and encourage sparsity\n",
    "    for i in range(num_weights):\n",
    "        Q[i, i] = abs(weights[i])  # Larger weights are penalized more\n",
    "    \n",
    "    return Q\n",
    "\n",
    "flattened_kernel_weights = binarized_kernel_weights.flatten()\n",
    "num_variables = min(len(flattened_kernel_weights), 600)  # Limit number of binary variables to 600\n",
    "selected_weights = flattened_kernel_weights[:num_variables]\n",
    "Q = generate_qubo_matrix(selected_weights)\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SimulatedAnnealingOptimizer()\n",
    "\n",
    "# Solve the QUBO problem\n",
    "solution = solver.solve(Q)\n",
    "best_solution = solution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_weights = np.array([selected_weights[i] if best_solution[i] == 1 else 0 for i in range(len(selected_weights))])\n",
    "\n",
    "# Update the model with the pruned weights\n",
    "pruned_kernel_weights = np.copy(binarized_kernel_weights)\n",
    "pruned_kernel_weights.flatten()[:num_variables] = pruned_weights  # Apply the pruned weights\n",
    "# pruned_kernel_weights.flatten()[selection] = pruned_weights  # Apply the pruned weights\n",
    "\n",
    "model.layers[0].set_weights([np.reshape(pruned_kernel_weights, model.layers[0].get_weights()[0].shape), bias_weights])\n",
    "\n",
    "# Optionally fine-tune the pruned model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define the deep learning model (a simple CNN)\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a function to create a more complex QUBO matrix\n",
    "def create_complex_qubo(weights, sparsity_factor=0.1, interaction_factor=0.01, entropy_factor=0.1):\n",
    "    # n_weights = len(weights)\n",
    "    n_weights = 600\n",
    "    Q = np.zeros((n_weights, n_weights))\n",
    "    \n",
    "    # Add sparsity regularization (penalize large weights, encourage sparsity)\n",
    "    for i in range(n_weights):\n",
    "        Q[i, i] += sparsity_factor * weights[i] ** 2  # Penalize non-zero weights\n",
    "    \n",
    "    # Add interaction regularization (encourage some weights to be correlated or anticorrelated)\n",
    "    for i in range(n_weights):\n",
    "        for j in range(i + 1, n_weights):\n",
    "            Q[i, j] += interaction_factor * weights[i] * weights[j]  # Penalize uncorrelated weights\n",
    "    \n",
    "    # Add entropy minimization (encourage confident predictions)\n",
    "    for i in range(n_weights):\n",
    "        for j in range(i + 1, n_weights):\n",
    "            Q[i, j] += entropy_factor * (weights[i] - 0.5) * (weights[j] - 0.5)  # Enforce weights to be either 0 or 1\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Define the energy function for QUBO\n",
    "def qubo_energy(x, Q):\n",
    "    return np.dot(x.T, np.dot(Q, x))  # Calculate the energy (objective function)\n",
    "\n",
    "# Initialize the model and get the weights\n",
    "model = create_model()\n",
    "initial_weights = []\n",
    "initial_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        kernel, bias = layer.get_weights()\n",
    "        initial_weights.append(kernel.flatten())  # Flatten kernels (weights)\n",
    "        initial_biases.append(bias.flatten())    # Flatten biases\n",
    "\n",
    "# Concatenate the initial weights and biases\n",
    "initial_weights = np.concatenate(initial_weights)\n",
    "initial_biases = np.concatenate(initial_biases)\n",
    "\n",
    "\n",
    "# Create a more complex QUBO matrix\n",
    "binary_weights_all = np.zeros(1)\n",
    "for i in range(int(initial_weights.shape[0] / 600)):\n",
    "    Q = create_complex_qubo(initial_weights)\n",
    "    solver = SimulatedAnnealingOptimizer()\n",
    "\n",
    "    # Solve the QUBO problem\n",
    "    solution = solver.solve(Q)\n",
    "    best_solution = solution[0]\n",
    "    \n",
    "    optimized_x = best_solution\n",
    "    binary_weights = np.zeros(600)\n",
    "    binary_weights[optimized_x == 1] = initial_weights[i*600:i*600+600][optimized_x == 1]\n",
    "    binary_weights_all = np.concatenate((binary_weights_all, binary_weights))\n",
    "\n",
    "binary_weights_all_new = binary_weights_all[1:]\n",
    "binary_weights_all_new = np.concatenate((binary_weights_all_new, np.array([0] * (len(initial_weights) - len(binary_weights_all_new)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 15, 15, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               295040    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 315722 (1.20 MB)\n",
      "Trainable params: 315722 (1.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct weights and biases for each layer\n",
    "layer_idx_w = 0\n",
    "layer_idx_b = 0\n",
    "updated_weights = []\n",
    "updated_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        num_params = layer.get_weights()[0].size  # Kernel size\n",
    "        num_biases = layer.get_weights()[1].size  # Bias size\n",
    "\n",
    "        # Get the kernel weights for the layer\n",
    "        layer_kernel = binary_weights_all_new[layer_idx_w:layer_idx_w + num_params].reshape(layer.get_weights()[0].shape)\n",
    "        updated_weights.append(layer_kernel)\n",
    "\n",
    "        # Get the biases for the layer\n",
    "        # layer_bias = binary_weights_all_new[layer_idx + num_params:layer_idx + num_params + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        layer_bias = initial_biases[layer_idx_b:layer_idx_b + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        updated_biases.append(layer_bias)\n",
    "\n",
    "        # Move to the next set of weights\n",
    "        # layer_idx += num_params + num_biases\n",
    "        layer_idx_w += num_params\n",
    "        layer_idx_b += num_biases\n",
    "\n",
    "# Now set the weights and biases to the model\n",
    "updated_weights = [w for w in updated_weights]\n",
    "updated_biases = [b for b in updated_biases]\n",
    "\n",
    "# Set the new weights and biases for the model\n",
    "model.set_weights([w for pair in zip(updated_weights, updated_biases) for w in pair])\n",
    "\n",
    "# Verify the model summary after setting the weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.8721 - accuracy: 0.6985 - val_loss: 0.9224 - val_accuracy: 0.6854\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.8116 - accuracy: 0.7188 - val_loss: 0.9131 - val_accuracy: 0.6830\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.7592 - accuracy: 0.7380 - val_loss: 0.8808 - val_accuracy: 0.6974\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.7123 - accuracy: 0.7540 - val_loss: 0.8825 - val_accuracy: 0.7001\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.6692 - accuracy: 0.7675 - val_loss: 0.8942 - val_accuracy: 0.7003\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.6276 - accuracy: 0.7830 - val_loss: 0.9391 - val_accuracy: 0.6890\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.5932 - accuracy: 0.7958 - val_loss: 0.8945 - val_accuracy: 0.7076\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.5575 - accuracy: 0.8060 - val_loss: 0.9117 - val_accuracy: 0.7030\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.5168 - accuracy: 0.8214 - val_loss: 0.9305 - val_accuracy: 0.7090\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.4846 - accuracy: 0.8305 - val_loss: 1.0188 - val_accuracy: 0.6928\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.0188 - accuracy: 0.6928\n",
      "Test accuracy: 69.28%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
