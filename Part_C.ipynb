{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import kaiwu as kw\n",
    "kw.license.init(user_id=\"72317291601100802\", sdk_code=\"vDSsMrcS1XvoHxrKEyWGPu3y6bydtx\")\n",
    "from kaiwu.classical import SimulatedAnnealingOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5206 - accuracy: 0.4450 - val_loss: 1.3459 - val_accuracy: 0.5274\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.1629 - accuracy: 0.5874 - val_loss: 1.1241 - val_accuracy: 0.5995\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.9935 - accuracy: 0.6514 - val_loss: 1.0394 - val_accuracy: 0.6336\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.8814 - accuracy: 0.6919 - val_loss: 0.9171 - val_accuracy: 0.6767\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.7897 - accuracy: 0.7242 - val_loss: 0.8597 - val_accuracy: 0.6993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define a simple CNN model for CIFAR-10\n",
    "def build_cnn_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = build_cnn_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (for initial weights)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Extract the trained weights (kernel and bias) from the first Conv2D layer\n",
    "conv2d_layer = model.layers[0]  # Assuming the first Conv2D layer\n",
    "kernel_weights, bias_weights = conv2d_layer.get_weights()  # Extract kernel and bias\n",
    "\n",
    "# binarized_kernel_weights = np.sign(kernel_weights)  # Binarize the kernel weights to +1 or -1\n",
    "# bias_weights will be left unchanged in this case\n",
    "binarized_kernel_weights = kernel_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Test Set Accuracy: 69.93%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)  # x_test is the CIFAR-10 test data\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # Get the index with the highest probability for each sample\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)  # Compare predicted vs. true labels\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qubo_matrix(weights,idx):\n",
    "    # num_weights = len(weights)\n",
    "    num_weights = 600\n",
    "    Q = np.zeros((num_weights, num_weights))\n",
    "    \n",
    "    # Penalize large weights and encourage sparsity\n",
    "    for i in range(num_weights):\n",
    "        Q[i, i] = abs(weights[i+idx])  # Larger weights are penalized more\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "initial_weights = []\n",
    "initial_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        kernel, bias = layer.get_weights()\n",
    "        initial_weights.append(kernel.flatten())  # Flatten kernels (weights)\n",
    "        initial_biases.append(bias.flatten())    # Flatten biases\n",
    "\n",
    "# Concatenate the initial weights and biases\n",
    "initial_weights = np.concatenate(initial_weights)\n",
    "initial_biases = np.concatenate(initial_biases)\n",
    "\n",
    "binary_weights_all = np.zeros(1)\n",
    "for i in range(int(initial_weights.shape[0] / 600)):\n",
    "    idx = i*600\n",
    "    Q = Q = generate_qubo_matrix(initial_weights,idx)\n",
    "    solver = SimulatedAnnealingOptimizer()\n",
    "\n",
    "    # Solve the QUBO problem\n",
    "    solution = solver.solve(Q)\n",
    "    best_solution = solution[0]\n",
    "    \n",
    "    optimized_x = best_solution\n",
    "    binary_weights = np.zeros(600)\n",
    "    binary_weights[optimized_x == 1] = initial_weights[i*600:i*600+600][optimized_x == 1]\n",
    "    binary_weights_all = np.concatenate((binary_weights_all, binary_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_weights_all_new = binary_weights_all[1:]\n",
    "binary_weights_all_new = np.concatenate((binary_weights_all_new, np.array([0] * (len(initial_weights) - len(binary_weights_all_new)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 15, 15, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356810 (1.36 MB)\n",
      "Trainable params: 356810 (1.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct weights and biases for each layer\n",
    "layer_idx_w = 0\n",
    "layer_idx_b = 0\n",
    "updated_weights = []\n",
    "updated_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        num_params = layer.get_weights()[0].size  # Kernel size\n",
    "        num_biases = layer.get_weights()[1].size  # Bias size\n",
    "\n",
    "        # Get the kernel weights for the layer\n",
    "        layer_kernel = binary_weights_all_new[layer_idx_w:layer_idx_w + num_params].reshape(layer.get_weights()[0].shape)\n",
    "        updated_weights.append(layer_kernel)\n",
    "\n",
    "        # Get the biases for the layer\n",
    "        # layer_bias = binary_weights_all_new[layer_idx + num_params:layer_idx + num_params + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        layer_bias = initial_biases[layer_idx_b:layer_idx_b + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        updated_biases.append(layer_bias)\n",
    "\n",
    "        # Move to the next set of weights\n",
    "        # layer_idx += num_params + num_biases\n",
    "        layer_idx_w += num_params\n",
    "        layer_idx_b += num_biases\n",
    "\n",
    "# Now set the weights and biases to the model\n",
    "updated_weights = [w for w in updated_weights]\n",
    "updated_biases = [b for b in updated_biases]\n",
    "\n",
    "# Set the new weights and biases for the model\n",
    "model.set_weights([w for pair in zip(updated_weights, updated_biases) for w in pair])\n",
    "\n",
    "# Verify the model summary after setting the weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.0850 - accuracy: 0.6164 - val_loss: 0.9775 - val_accuracy: 0.6595\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.8193 - accuracy: 0.7144 - val_loss: 0.8689 - val_accuracy: 0.6934\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.7117 - accuracy: 0.7517 - val_loss: 0.8322 - val_accuracy: 0.7081\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.6484 - accuracy: 0.7723 - val_loss: 0.8026 - val_accuracy: 0.7248\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.5761 - accuracy: 0.7992 - val_loss: 0.8128 - val_accuracy: 0.7248\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.8128 - accuracy: 0.7248\n",
      "Test accuracy: 72.48%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "600\n",
      "1200\n",
      "1800\n",
      "2400\n",
      "3000\n",
      "3600\n",
      "4200\n",
      "4800\n",
      "5400\n",
      "6000\n",
      "6600\n",
      "7200\n",
      "7800\n",
      "8400\n",
      "9000\n",
      "9600\n",
      "10200\n",
      "10800\n",
      "11400\n",
      "12000\n",
      "12600\n",
      "13200\n",
      "13800\n",
      "14400\n",
      "15000\n",
      "15600\n",
      "16200\n",
      "16800\n",
      "17400\n",
      "18000\n",
      "18600\n",
      "19200\n",
      "19800\n",
      "20400\n",
      "21000\n",
      "21600\n",
      "22200\n",
      "22800\n",
      "23400\n",
      "24000\n",
      "24600\n",
      "25200\n",
      "25800\n",
      "26400\n",
      "27000\n",
      "27600\n",
      "28200\n",
      "28800\n",
      "29400\n",
      "30000\n",
      "30600\n",
      "31200\n",
      "31800\n",
      "32400\n",
      "33000\n",
      "33600\n",
      "34200\n",
      "34800\n",
      "35400\n",
      "36000\n",
      "36600\n",
      "37200\n",
      "37800\n",
      "38400\n",
      "39000\n",
      "39600\n",
      "40200\n",
      "40800\n",
      "41400\n",
      "42000\n",
      "42600\n",
      "43200\n",
      "43800\n",
      "44400\n",
      "45000\n",
      "45600\n",
      "46200\n",
      "46800\n",
      "47400\n",
      "48000\n",
      "48600\n",
      "49200\n",
      "49800\n",
      "50400\n",
      "51000\n",
      "51600\n",
      "52200\n",
      "52800\n",
      "53400\n",
      "54000\n",
      "54600\n",
      "55200\n",
      "55800\n",
      "56400\n",
      "57000\n",
      "57600\n",
      "58200\n",
      "58800\n",
      "59400\n",
      "60000\n",
      "60600\n",
      "61200\n",
      "61800\n",
      "62400\n",
      "63000\n",
      "63600\n",
      "64200\n",
      "64800\n",
      "65400\n",
      "66000\n",
      "66600\n",
      "67200\n",
      "67800\n",
      "68400\n",
      "69000\n",
      "69600\n",
      "70200\n",
      "70800\n",
      "71400\n",
      "72000\n",
      "72600\n",
      "73200\n",
      "73800\n",
      "74400\n",
      "75000\n",
      "75600\n",
      "76200\n",
      "76800\n",
      "77400\n",
      "78000\n",
      "78600\n",
      "79200\n",
      "79800\n",
      "80400\n",
      "81000\n",
      "81600\n",
      "82200\n",
      "82800\n",
      "83400\n",
      "84000\n",
      "84600\n",
      "85200\n",
      "85800\n",
      "86400\n",
      "87000\n",
      "87600\n",
      "88200\n",
      "88800\n",
      "89400\n",
      "90000\n",
      "90600\n",
      "91200\n",
      "91800\n",
      "92400\n",
      "93000\n",
      "93600\n",
      "94200\n",
      "94800\n",
      "95400\n",
      "96000\n",
      "96600\n",
      "97200\n",
      "97800\n",
      "98400\n",
      "99000\n",
      "99600\n",
      "100200\n",
      "100800\n",
      "101400\n",
      "102000\n",
      "102600\n",
      "103200\n",
      "103800\n",
      "104400\n",
      "105000\n",
      "105600\n",
      "106200\n",
      "106800\n",
      "107400\n",
      "108000\n",
      "108600\n",
      "109200\n",
      "109800\n",
      "110400\n",
      "111000\n",
      "111600\n",
      "112200\n",
      "112800\n",
      "113400\n",
      "114000\n",
      "114600\n",
      "115200\n",
      "115800\n",
      "116400\n",
      "117000\n",
      "117600\n",
      "118200\n",
      "118800\n",
      "119400\n",
      "120000\n",
      "120600\n",
      "121200\n",
      "121800\n",
      "122400\n",
      "123000\n",
      "123600\n",
      "124200\n",
      "124800\n",
      "125400\n",
      "126000\n",
      "126600\n",
      "127200\n",
      "127800\n",
      "128400\n",
      "129000\n",
      "129600\n",
      "130200\n",
      "130800\n",
      "131400\n",
      "132000\n",
      "132600\n",
      "133200\n",
      "133800\n",
      "134400\n",
      "135000\n",
      "135600\n",
      "136200\n",
      "136800\n",
      "137400\n",
      "138000\n",
      "138600\n",
      "139200\n",
      "139800\n",
      "140400\n",
      "141000\n",
      "141600\n",
      "142200\n",
      "142800\n",
      "143400\n",
      "144000\n",
      "144600\n",
      "145200\n",
      "145800\n",
      "146400\n",
      "147000\n",
      "147600\n",
      "148200\n",
      "148800\n",
      "149400\n",
      "150000\n",
      "150600\n",
      "151200\n",
      "151800\n",
      "152400\n",
      "153000\n",
      "153600\n",
      "154200\n",
      "154800\n",
      "155400\n",
      "156000\n",
      "156600\n",
      "157200\n",
      "157800\n",
      "158400\n",
      "159000\n",
      "159600\n",
      "160200\n",
      "160800\n",
      "161400\n",
      "162000\n",
      "162600\n",
      "163200\n",
      "163800\n",
      "164400\n",
      "165000\n",
      "165600\n",
      "166200\n",
      "166800\n",
      "167400\n",
      "168000\n",
      "168600\n",
      "169200\n",
      "169800\n",
      "170400\n",
      "171000\n",
      "171600\n",
      "172200\n",
      "172800\n",
      "173400\n",
      "174000\n",
      "174600\n",
      "175200\n",
      "175800\n",
      "176400\n",
      "177000\n",
      "177600\n",
      "178200\n",
      "178800\n",
      "179400\n",
      "180000\n",
      "180600\n",
      "181200\n",
      "181800\n",
      "182400\n",
      "183000\n",
      "183600\n",
      "184200\n",
      "184800\n",
      "185400\n",
      "186000\n",
      "186600\n",
      "187200\n",
      "187800\n",
      "188400\n",
      "189000\n",
      "189600\n",
      "190200\n",
      "190800\n",
      "191400\n",
      "192000\n",
      "192600\n",
      "193200\n",
      "193800\n",
      "194400\n",
      "195000\n",
      "195600\n",
      "196200\n",
      "196800\n",
      "197400\n",
      "198000\n",
      "198600\n",
      "199200\n",
      "199800\n",
      "200400\n",
      "201000\n",
      "201600\n",
      "202200\n",
      "202800\n",
      "203400\n",
      "204000\n",
      "204600\n",
      "205200\n",
      "205800\n",
      "206400\n",
      "207000\n",
      "207600\n",
      "208200\n",
      "208800\n",
      "209400\n",
      "210000\n",
      "210600\n",
      "211200\n",
      "211800\n",
      "212400\n",
      "213000\n",
      "213600\n",
      "214200\n",
      "214800\n",
      "215400\n",
      "216000\n",
      "216600\n",
      "217200\n",
      "217800\n",
      "218400\n",
      "219000\n",
      "219600\n",
      "220200\n",
      "220800\n",
      "221400\n",
      "222000\n",
      "222600\n",
      "223200\n",
      "223800\n",
      "224400\n",
      "225000\n",
      "225600\n",
      "226200\n",
      "226800\n",
      "227400\n",
      "228000\n",
      "228600\n",
      "229200\n",
      "229800\n",
      "230400\n",
      "231000\n",
      "231600\n",
      "232200\n",
      "232800\n",
      "233400\n",
      "234000\n",
      "234600\n",
      "235200\n",
      "235800\n",
      "236400\n",
      "237000\n",
      "237600\n",
      "238200\n",
      "238800\n",
      "239400\n",
      "240000\n",
      "240600\n",
      "241200\n",
      "241800\n",
      "242400\n",
      "243000\n",
      "243600\n",
      "244200\n",
      "244800\n",
      "245400\n",
      "246000\n",
      "246600\n",
      "247200\n",
      "247800\n",
      "248400\n",
      "249000\n",
      "249600\n",
      "250200\n",
      "250800\n",
      "251400\n",
      "252000\n",
      "252600\n",
      "253200\n",
      "253800\n",
      "254400\n",
      "255000\n",
      "255600\n",
      "256200\n",
      "256800\n",
      "257400\n",
      "258000\n",
      "258600\n",
      "259200\n",
      "259800\n",
      "260400\n",
      "261000\n",
      "261600\n",
      "262200\n",
      "262800\n",
      "263400\n",
      "264000\n",
      "264600\n",
      "265200\n",
      "265800\n",
      "266400\n",
      "267000\n",
      "267600\n",
      "268200\n",
      "268800\n",
      "269400\n",
      "270000\n",
      "270600\n",
      "271200\n",
      "271800\n",
      "272400\n",
      "273000\n",
      "273600\n",
      "274200\n",
      "274800\n",
      "275400\n",
      "276000\n",
      "276600\n",
      "277200\n",
      "277800\n",
      "278400\n",
      "279000\n",
      "279600\n",
      "280200\n",
      "280800\n",
      "281400\n",
      "282000\n",
      "282600\n",
      "283200\n",
      "283800\n",
      "284400\n",
      "285000\n",
      "285600\n",
      "286200\n",
      "286800\n",
      "287400\n",
      "288000\n",
      "288600\n",
      "289200\n",
      "289800\n",
      "290400\n",
      "291000\n",
      "291600\n",
      "292200\n",
      "292800\n",
      "293400\n",
      "294000\n",
      "294600\n",
      "295200\n",
      "295800\n",
      "296400\n",
      "297000\n",
      "297600\n",
      "298200\n",
      "298800\n",
      "299400\n",
      "300000\n",
      "300600\n",
      "301200\n",
      "301800\n",
      "302400\n",
      "303000\n",
      "303600\n",
      "304200\n",
      "304800\n",
      "305400\n",
      "306000\n",
      "306600\n",
      "307200\n",
      "307800\n",
      "308400\n",
      "309000\n",
      "309600\n",
      "310200\n",
      "310800\n",
      "311400\n",
      "312000\n",
      "312600\n",
      "313200\n",
      "313800\n",
      "314400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define the deep learning model (a simple CNN)\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a function to create a more complex QUBO matrix\n",
    "\n",
    "\n",
    "def create_complex_qubo(weights, idx,sparsity_factor=0.1, interaction_factor=0.01, entropy_factor=0.1):\n",
    "    # n_weights = len(weights)\n",
    "    n_weights = 600\n",
    "    Q = np.zeros((n_weights, n_weights))\n",
    "    print(idx)\n",
    "    \n",
    "    # Add sparsity regularization (penalize large weights, encourage sparsity)\n",
    "    for i in range(n_weights):\n",
    "        Q[i, i] += sparsity_factor * weights[i+idx] ** 2  # Penalize non-zero weights\n",
    "    \n",
    "    # Add interaction regularization (encourage some weights to be correlated or anticorrelated)\n",
    "    for i in range(n_weights):\n",
    "        for j in range(i + 1, n_weights):\n",
    "            Q[i, j] += interaction_factor * weights[i+idx] * weights[j+idx]  # Penalize uncorrelated weights\n",
    "    \n",
    "    # Add entropy minimization (encourage confident predictions)\n",
    "    for i in range(n_weights):\n",
    "        for j in range(i + 1, n_weights):\n",
    "            Q[i, j] += entropy_factor * (weights[i+idx] - 0.5) * (weights[j+idx] - 0.5)  # Enforce weights to be either 0 or 1\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Define the energy function for QUBO\n",
    "def qubo_energy(x, Q):\n",
    "    return np.dot(x.T, np.dot(Q, x))  # Calculate the energy (objective function)\n",
    "\n",
    "# Initialize the model and get the weights\n",
    "model = create_model()\n",
    "initial_weights = []\n",
    "initial_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        kernel, bias = layer.get_weights()\n",
    "        initial_weights.append(kernel.flatten())  # Flatten kernels (weights)\n",
    "        initial_biases.append(bias.flatten())    # Flatten biases\n",
    "\n",
    "# Concatenate the initial weights and biases\n",
    "initial_weights = np.concatenate(initial_weights)\n",
    "initial_biases = np.concatenate(initial_biases)\n",
    "\n",
    "\n",
    "# Create a more complex QUBO matrix\n",
    "binary_weights_all = np.zeros(1)\n",
    "for i in range(int(initial_weights.shape[0] / 600)):\n",
    "    idx = i*600\n",
    "    Q = create_complex_qubo(initial_weights,idx)\n",
    "    solver = SimulatedAnnealingOptimizer()\n",
    "\n",
    "    # Solve the QUBO problem\n",
    "    solution = solver.solve(Q)\n",
    "    best_solution = solution[0]\n",
    "    \n",
    "    optimized_x = best_solution\n",
    "    binary_weights = np.zeros(600)\n",
    "    binary_weights[optimized_x == 1] = initial_weights[i*600:i*600+600][optimized_x == 1]\n",
    "    binary_weights_all = np.concatenate((binary_weights_all, binary_weights))\n",
    "\n",
    "binary_weights_all_new = binary_weights_all[1:]\n",
    "binary_weights_all_new = np.concatenate((binary_weights_all_new, np.array([0] * (len(initial_weights) - len(binary_weights_all_new)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 315722 (1.20 MB)\n",
      "Trainable params: 315722 (1.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct weights and biases for each layer\n",
    "layer_idx_w = 0\n",
    "layer_idx_b = 0\n",
    "updated_weights = []\n",
    "updated_biases = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D) or isinstance(layer, layers.Dense):\n",
    "        num_params = layer.get_weights()[0].size  # Kernel size\n",
    "        num_biases = layer.get_weights()[1].size  # Bias size\n",
    "\n",
    "        # Get the kernel weights for the layer\n",
    "        layer_kernel = binary_weights_all_new[layer_idx_w:layer_idx_w + num_params].reshape(layer.get_weights()[0].shape)\n",
    "        updated_weights.append(layer_kernel)\n",
    "\n",
    "        # Get the biases for the layer\n",
    "        # layer_bias = binary_weights_all_new[layer_idx + num_params:layer_idx + num_params + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        layer_bias = initial_biases[layer_idx_b:layer_idx_b + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        updated_biases.append(layer_bias)\n",
    "\n",
    "        # Move to the next set of weights\n",
    "        # layer_idx += num_params + num_biases\n",
    "        layer_idx_w += num_params\n",
    "        layer_idx_b += num_biases\n",
    "\n",
    "# Now set the weights and biases to the model\n",
    "updated_weights = [w for w in updated_weights]\n",
    "updated_biases = [b for b in updated_biases]\n",
    "\n",
    "# Set the new weights and biases for the model\n",
    "model.set_weights([w for pair in zip(updated_weights, updated_biases) for w in pair])\n",
    "\n",
    "# Verify the model summary after setting the weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.6383 - accuracy: 0.3953 - val_loss: 1.3699 - val_accuracy: 0.5031\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.3053 - accuracy: 0.5338 - val_loss: 1.2174 - val_accuracy: 0.5742\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.1414 - accuracy: 0.5979 - val_loss: 1.0696 - val_accuracy: 0.6296\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.0310 - accuracy: 0.6406 - val_loss: 1.0255 - val_accuracy: 0.6466\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9503 - accuracy: 0.6676 - val_loss: 1.0283 - val_accuracy: 0.6442\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8883 - accuracy: 0.6918 - val_loss: 0.9249 - val_accuracy: 0.6790\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8272 - accuracy: 0.7117 - val_loss: 0.9096 - val_accuracy: 0.6871\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7787 - accuracy: 0.7283 - val_loss: 0.8999 - val_accuracy: 0.6956\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.7352 - accuracy: 0.7442 - val_loss: 0.9146 - val_accuracy: 0.6841\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.6915 - accuracy: 0.7606 - val_loss: 0.9021 - val_accuracy: 0.6937\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.9021 - accuracy: 0.6937\n",
      "Test accuracy: 69.37%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
